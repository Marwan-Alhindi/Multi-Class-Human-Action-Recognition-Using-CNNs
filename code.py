# -*- coding: utf-8 -*-
"""Copy of A1_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bn0m9aFv7IVgw1k-wvxju9kQdzzCru_l

# Table Of Contents

* [Libaries and their usages](#Libaries-and-their-usages)
* [Exploratory Data Analysis: Becoming one with the data](#Exploratory-Data-Analysis:-Becoming-one-with-the-data)
    * [General Data Quality](#General-Data-Quality)
    * [Image size and aspect ratios](#Image-size-and-aspect-ratios)
    * [Identiyfing class imbalances](#Identiyfing-class-imbalances)
* [Preprocessing](#Preprocessing)
    * [Preprocess dataframe](#Preprocess-dataframe)
    * [Understanding Images Arrays](#Understanding-Images-Arrays)
    * [Training, validation and test sets](#Training,-validation-and-test-sets)
* [Model training and evaluating](#Model-training-and-evaluating)
    * [Why CNN for images?](#Why-CNN-for-images?)
    * [Baseline Model](#Baseline-Model)
    * [Why Not Just Increase Neural Network Depth](#Why-Not-Just-Increase-Neural-Network-Depth)
    * [Overfitting model](#Overfitting-model)
        * [ResNet50](#ResNet50)
        * [VGG16](#VGG16)
        * [MobileNetV2](#MobileNetV2)
        * [NasNetLarge](#NasNetLarge)
* [MobileNet Regularization](#MMobileNet-Regularization)
  * [Increasing data size via data augemntation](#Increasing-data-size-via-data-augemntation)
  * [dropouts, L1 & L2](#dropouts,-L1-&-L2)
  * [Optimizer regularization](#Optimizer-regularization)
* [Saving the final model](#Saving-the-final-model)
* [Predictions](#Predictions)
* [Conclusion](#Conclusion)
* [References](#References)

# Libraries
"""

# Load necessary libraries and modules
import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layers, models, optimizers, regularizers
from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint
import tensorflow as tf
from tensorflow.keras.applications import MobileNet, NASNetMobile, NASNetLarge, MobileNetV2, VGG16, ResNet50
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from PIL import Image
from google.colab import drive
import datetime
import json

"""# Exploratory Data Analysis: Becoming one with the data"""

# Mount Google Drive
drive.mount('/content/drive')

# Copy the zip file from Google Drive to the current working directory in Colab
!cp '/content/drive/My Drive/Images.zip' .

# Unzip the file
!unzip -q -o Images.zip

# Remove the zip file to clean up
!rm Images.zip

# List the contents of the directory to verify
!ls

# Mount Google Drive
drive.mount('/content/drive')

# Copy the zip file from Google Drive to the current working directory in Colab
!cp '/content/drive/My Drive/A1_2024_data.zip' .

# Unzip the file
!unzip -q -o A1_2024_data.zip

# Remove the zip file to clean up
!rm A1_2024_data.zip

# List the contents of the extracted directory to verify
!ls

"""The first step to training a neural net is to avoid diving into neural net code right away. Instead, we begin by thoroughly inspecting the data. This step is critical. It is essential to spend a significant amount of time (often measured in hours) scanning through thousands of examples, understanding their distribution, and looking for patterns. The human brain is naturally adept at this task. For instance, one might discover that the data contains duplicate examples or find corrupted images or labels. It’s also important to look for data imbalances and biases. Additionally, paying attention to the process of classifying the data can provide insights into the types of architectures that might be explored later. For example, are very local features sufficient, or is global context necessary? How much variation exists, and in what form? What variation is spurious and could be preprocessed out? Does spatial position matter, or should it be averaged out? How much does detail matter, and to what extent can the images be downsampled? How noisy are the labels?

What do we need to know to make sure our CNN will perform as well as possible? And what do we need to know as data scientists about our data to better understand the model itself?

Data exploration is key to a lot of machine learning processes. That said, when it comes to object detection and image segmentation datasets there is no straightforward way to systematically do data exploration. There are multiple things that distinguish working with regular image datasets from object and segmentation ones:

- The label is strongly bound to the image. Suddenly we have to be careful of whatever we do to our images as it can break the image-label-mapping.
- Usually much more labels per image.
- Much more hyperparameters to tune (especially if we train on custom datasets)

The first thing we should do when working on any machine learning problem (image segmentation, object detection included)  is assessing quality and understanding our data.

Common data problems when training Object Detection and Image Segmentation models include:

- Image dimensions and aspect ratios (especially dealing with extreme values)
- Labels composition – imbalances, bounding box sizes, aspect ratios (for instance a lot of small objects)
- Data preparation not suitable for our dataset.
- Modelling approach not aligned with the data.

## General Data Quality

This one is simple and rather obvious, also this step would be the same for all image problems not just object detection or image segmentation. What we need to do here is:

- we get the general feel of a dataset and inspect it visually.
- we make sure it’s not corrupt and  does not contain any obvious artifacts (for instance  black only images)  

We want to visualize as many images as possible. There are multiple ways of doing this:

- Plot them in a jupyter notebook using matplotlib.
- Use dedicated tooling like google facets to explore image data
- Use HTML rendering to visualize and explore in a notebook.

I’m a huge fan of the last option, it works great in jupyter notebooks (even for thousands of pictures at the same time!)
"""

pip install easyimages

from easyimages import EasyImageList

Li = EasyImageList.from_folder('/content/drive/My Drive/A1_2024_data/Images/')
Li.symlink_images()
Li.html(sample = 500, size = 44)

"""## Image size and aspect ratios

Datasets are unlikely to contain images of the same  sizes and aspect ratios. Inspecting basic datasets statistics such as aspect ratios, image widths and heights will help us make important decisions:

- Can we and should we? do destructive resizing? (destructive means resizing that changes the AR)
- For non-destructive resizing what should be our desired output resolution and amount of padding?
- Deep Learning models might have hyper parameters we have to tune depending on the above  (for instance anchor size and ratios)  or they might even have strong requirements when it comes to minimum input image size.


In general we would expect most datasets to fall into one of 3 categories:

- Uniformly distributed where most of the images have the same dimensions – here the only decision we will have to make is how much to resize (if at all). This will mainly depend on objects area, size and aspect ratios.
- Slightly bimodal distribution but most of the images are in the aspect ratio range of (0.7  …  1.5). For those type of datasets we should be fine by going with a non-destructive resize -> Pad approach. Padding will be necessary but to a degree that is manageable and will not blow the size of the dataset too much.
- Dataset with a lot of extreme values (very wide images mixed with very narrow ones) – this case is much more tricky and there are more advanced techniques to avoid excessive padding. We might consider sampling batches of images based on the aspect ratio. But this can introduce a bias to our sampling process – so we need to make sure its acceptable or not strong enough.
"""

# load the training data
training_data = pd.read_csv('A1_2024_data/train_data_2024.csv')
training_data.head()

# Commented out IPython magic to ensure Python compatibility.
# Ensure plots are displayed inline in the notebook
# %matplotlib inline

# Path to the directory containing your images
image_dir = 'Images/'

# List to store aspect ratios
aspect_ratios = []

# Iterate through each file in the directory
for filename in os.listdir(image_dir):
    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):
        # Open the image
        img = Image.open(os.path.join(image_dir, filename))
        # Calculate aspect ratio (width / height)
        aspect_ratio = img.width / img.height
        # Append to the list
        aspect_ratios.append(aspect_ratio)

# Plotting the histogram with more bins
plt.hist(aspect_ratios, bins=50, color='blue', alpha=0.7)

# Setting the x-ticks to show more precision, with a step of 0.2
plt.xticks(np.arange(0, 3, 0.2))

plt.xlabel('Aspect Ratio')
plt.ylabel('Count')
plt.title('Histogram of Image Aspect Ratios')

# Display the plot inline in the notebook
plt.show()

"""An aspect ratio of 1.0 corresponds to square images (width and height are the same), while an aspect ratio of 1.5 is typical of images that are slightly wider than they are tall.

- most of the images are wider than taller although there are some peaks also from 0.5 to 1 which indicates images that are taller.


If we plan on resizing images for model input, knowing the common aspect ratios can help you decide on a target aspect ratio that minimizes distortion. For example, if most images have an aspect ratio of around 1.4 and 1.5, resizing images while maintaining this aspect ratio might be beneficial.
"""

image_dir = 'Images'

widths = []
heights = []

for filename in os.listdir(image_dir):
    img = Image.open(os.path.join(image_dir, filename))
    widths.append(img.width)
    heights.append(img.height)

plt.figure(figsize=(10, 5))
plt.hist(widths, bins=50, alpha=0.5, label='Widths')
plt.hist(heights, bins=50, alpha=0.5, label='Heights')
plt.legend(loc='upper right')
plt.title('Distribution of Image Dimensions')
plt.show()

"""- There are clear peaks at certain widths and heights, suggesting that a significant portion of your dataset consists of images with specific dimensions. For example, one prominent peak appears around 300 pixels wide and 300 pixels high, indicating that many images in your dataset are square and around this size.
- Another noticeable peak is around 400 pixels wide, with a slightly smaller height, suggesting that some images are slightly wider than they are tall.

Resizing: Given the variation in image dimensions, we may need to resize your images to a standard size before feeding them into a model. Since many images are around 300 pixels, this might be a reasonable target size for resizing.
"""

# lets get the average width and height
avg_width = int(np.mean(widths))
avg_height = int(np.mean(heights))
print(avg_width, avg_height)

"""## Identiyfing class imbalances"""

training_data['Class'].value_counts() / len(training_data)

# lets plot the class distribution
training_data['Class'].value_counts().plot(kind='bar')

"""Due to the low percentage of data for each class, a model might struggle to learn the distinguishing features of each class, especially for those with the least representation. This might result in the model being biased towards the more frequent classes.

To address the class imbalance, techniques such as data augmentation (to artificially increase the size of the smaller classes) or resampling (either oversampling the minority classes or undersampling the majority classes) may be necessary to help the model learn more effectively.
"""

# now for the other target variable
training_data['MoreThanOnePerson'].value_counts() / len(training_data)

"""The target variable is moderately imbalanced, with the "NO" class being more prevalent than the "YES" class. This indicates that in most of the data, there is only one person present, while in the remaining data, there are multiple people.

The model might be slightly biased towards predicting "NO" due to its higher prevalence in the training data.

We might consider techniques like resampling (oversampling the "YES" class or undersampling the "NO" class) or using class weights in the model to balance the influence of both classes during training.

# Preprocessing

In this section, we will create the training and test data.

## Preprocess dataframe
"""

# filter the data to get the images that are not in the training data
images = os.listdir('Images')
print(len(images))

len(training_data)

"""- we have 4500 labeled images and 9532 - 4500 = 5032 unlabeled."""

def load_images_from_folder(folder, training_data_filenames):

    # training images
    training_images = []
    training_images_filenames = []

    # the remaining unlabeled images from the Images folder
    unlabeled_images = []
    unlabeled_images_filenames = []

    # for every image in the folder
    for filename in os.listdir(folder):
        # read the image
        img = cv2.imread(os.path.join(folder, filename))

        # if it exists (just to make sure)
        if img is not None:
            # Convert from BGR to RGB (standard cv2 is to read BGR)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            # Standardize the image (resize and scale pixel values)
            img = standardize_image(img)

            # Filter the image: if it belong to training or unlabeled
            if filename in training_data_filenames:
                training_images.append(img)
                training_images_filenames.append(filename)
            else:
                unlabeled_images.append(img)
                unlabeled_images_filenames.append(filename)

    return training_images, training_images_filenames, unlabeled_images, unlabeled_images_filenames

# Function to standardize images
def standardize_image(image):
    standardized_image = cv2.resize(image, (224, 224))  # Default of resnet size
    standardized_image = standardized_image / 255.0  # Scale pixel values to [0, 1]
    return standardized_image


# Load your CSV data
training_data = pd.read_csv('A1_2024_data/train_data_2024.csv')

# Extract the filenames from the DataFrame
training_data_filenames = training_data['FileName'].tolist()

# Load and standardize the images and their filenames
training_images, training_images_filenames, unlabeled_images, unlabeled_images_filenames = load_images_from_folder('Images', training_data_filenames)

# Create a DataFrame for the training images
training_images_df = pd.DataFrame({
    'FileName': training_images_filenames,
    'Image': training_images
})

# Merge the image DataFrame with the original training data to include labels
training_images_with_labels = pd.merge(training_images_df, training_data, on='FileName')

# Create a DataFrame for the unlabeled images
unlabeled_images_df = pd.DataFrame({
    'FileName': unlabeled_images_filenames,
    'Image': unlabeled_images,
    'Label': 'Unlabeled'  # Assign 'Unlabeled' as the label for all these images
})

"""- load_images_from_folder takes the folder name that contains all the images and training_data_filenames which are a subset of images from the folder. It outputs the training data with its filenames and the remaining unlabeled images with their filenames.
        training_images, training_images_filenames, unlabeled_images, unlabeled_images_filenames
- It also standardize the images to the same size and scale the pixels from 0 to 1.

Why do we need to standardize?
- **Consistency in Input Size:** Ensures all images have the same dimensions, which is required for feeding them into a neural network.
- **Normalization:** Scaling pixel values to [0, 1] improves convergence during model training by maintaining consistency in the range of input data.
- **Reduce Computational Load:** Reducing the variability in image sizes and pixel values helps the model learn more effectively.
- **Prevent Bias:** Avoids bias towards larger or differently scaled images, ensuring fair learning across the dataset.
- **Preprocessing Requirement:** Many pre-trained models and deep learning architectures expect standardized inputs.
"""

training_images_with_labels

"""- As expected, we get total of 4500 images."""

unlabeled_images_df

"""- As expected, we get 5032 total unlabeled images.

## Understanding Images Arrays

Lets analyze the Image array
"""

print(f"Shape of one image: {training_images_with_labels['Image'][0].shape}")
print(f"Entire Pixels of one image:\n{training_images_with_labels['Image'][0]}")

print(f"The first image row shape:{training_images_with_labels['Image'][0][0].shape}")
print(f"The first image pixels row:\n{training_images_with_labels['Image'][0][0]}")

print(f"The first pixel in the first row of the first image shape: {training_images_with_labels['Image'][0][0][0].shape}")
print(f"The first pixel in the first row of the first image:\n{training_images_with_labels['Image'][0][0][0]}")

"""- We have total of 4500 training instances
- Every image is 224 by 224 where every pixel is explained by three numbers to describe RGB.

Neural network models typically expect input data in the form of a NumPy array rather than a DataFrame. Lets now make the training data!

## Training, validation and test sets
"""

# Extract the images and labels
X = np.array(training_images_with_labels['Image'].tolist())  # Convert image list to a NumPy array
y = training_images_with_labels.drop(columns=['FileName', 'Image', 'HighLevelCategory'])  # Labels as DataFrame

# Convert labels to NumPy array
y_array = y.values

print(y_array)
print(f"y_array shape: {y_array.shape}")

X.shape

print(f"Shape of one image: {X[0].shape}")
print(f"The first image row shape :{X[0][0].shape}")
print(f"The first pixel in the first row of the first image shape: {X[0][0][0].shape}")

"""- shapes of X and y are as expected.

Lets see an image example:
"""

plt.imshow(X[0])

y['Class'][0]

y['MoreThanOnePerson'][0]

"""Lets do the splitting:

- Training Set: 2,700 instances (60% of the total 4,500)
- Validation Set: 900 instances (20% of the total 4,500)
- Test Set: 900 instances (20% of the total 4,500)
"""

# First split the dataset into training+validation and test sets
X_train_val, X_test, y_train_val, y_test = train_test_split(X, y_array, test_size=0.2, random_state=42)

# Now split the training+validation set into a separate training set and a validation set
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)

# X_train and y_train are used to train the model
# X_val and y_val are used to validate the model during training
# X_test and y_test are used to evaluate the final model

"""- random_state parameter ensures that the results of the data splitting are reproducible.

Neural networks works very well with floats and not any other data types.

First, we need to seperate the labels because currently the labels are given as follows: ['class','number_of_humans']

- Neural networks typically expect the output labels to correspond directly to the number of output neurons.
- Our task involves predicting two different types of outputs: one for the class of the activity and another for the number of humans in the image. These are conceptually different predictions and may require different types of processing in the network.
- If these outputs were kept together, the neural network would need to treat them as a single output, which complicates the learning process.

Then, we need to apply one hot encoding for the 'Class' target and binary encoding to 'MoreThanOnePerson' target class.

- Label encoding converts categorical labels into a numerical format that machine learning models can work with. It's a required step before applying one-hot encoding.
- Many neural networks require the target labels to be in one-hot encoded format when performing multi-class classification. This format makes it easier for the model to calculate loss and make predictions, as it outputs probabilities for each class.

Finally, we change and check if all datatypes are floats. Lets do this!
"""

# Separate the class and human presence targets
y_train_class = y_train[:, 0]
y_train_humans = y_train[:, 1]

y_val_class = y_val[:, 0]
y_val_humans = y_val[:, 1]

y_test_class = y_test[:, 0]
y_test_humans = y_test[:, 1]

# Apply label encoding to the 'Class' target
label_encoder_class = LabelEncoder()
y_train_class = label_encoder_class.fit_transform(y_train_class.astype(str))
y_val_class = label_encoder_class.transform(y_val_class.astype(str))
y_test_class = label_encoder_class.transform(y_test_class.astype(str))

# The mapping of integers to categories is stored in the 'classes_' attribute
categories = label_encoder_class.classes_

# Print the mapping (optional)
mapping = {index: category for index, category in enumerate(categories)}

# Now apply one-hot encoding to the encoded class labels
y_train_class_one_hot = to_categorical(y_train_class).astype(np.float32)
y_val_class_one_hot = to_categorical(y_val_class).astype(np.float32)
y_test_class_one_hot = to_categorical(y_test_class).astype(np.float32)

# Apply binary encoding to 'MoreThanOnePerson'
y_train_humans = (y_train_humans == 'YES').astype(np.float32)
y_val_humans = (y_val_humans == 'YES').astype(np.float32)
y_test_humans = (y_test_humans == 'YES').astype(np.float32)

# Ensure that X_train, X_val, and X_test are float32
X_train = np.array(X_train, dtype=np.float32)
X_val = np.array(X_val, dtype=np.float32)
X_test = np.array(X_test, dtype=np.float32)

mapping

# Print data types for the training set
print(f'X_train data type: {X_train.dtype}')
print(f'y_train_class_one_hot data type: {y_train_class_one_hot.dtype}')
print(f'y_train_humans data type: {y_train_humans.dtype}')

# Print data types for the validation set
print(f'X_val data type: {X_val.dtype}')
print(f'y_val_class_one_hot data type: {y_val_class_one_hot.dtype}')
print(f'y_val_humans data type: {y_val_humans.dtype}')

# Print data types for the test set
print(f'X_test data type: {X_test.dtype}')
print(f'y_test_class_one_hot data type: {y_test_class_one_hot.dtype}')
print(f'y_test_humans data type: {y_test_humans.dtype}')

"""We need to also check the number of instances if input (features) and output (target) matches."""

# Check the shape of the input data
print(f"X_train shape: {X_train.shape}")
print(f"y_train_class_one_hot shape: {y_train_class_one_hot.shape}")
print(f'y_train_humans shape: {y_train_humans.shape}')

print(f"X_val shape: {X_val.shape}")
print(f"y_val_class_one_hot shape: {y_val_class_one_hot.shape}")
print(f'y_val_humans shape: {y_val_humans.shape}')


print(f"X_test shape: {X_test.shape}")
print(f"y_test_class_one_hot shape: {y_test_class_one_hot.shape}")
print(f'y_test_humans shape: {y_test_humans.shape}')

"""This is as expected:

- We are expecting 2700 training instances with output of 40 classes for 'Class' target and output of 0 or 1 for 'MoreThanOneHuman' target.
- We are expecting 900 validation instances with output of 40 classes for 'Class' target and output of 0 or 1 for 'MoreThanOneHuman' target.
- We are expecting 900 test instances with output of 40 classes for 'Class' target and output of 0 or 1 for 'MoreThanOneHuman' target.

# Model training and evaluating

Now that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it is best to pick some simple model that we couldn’t possibly have screwed up somehow - e.g. a linear classifier, or a very tiny ConvNet. We’ll want to train it, visualize the losses, any other metrics (e.g. accuracy), model predictions, and perform a series of ablation experiments with explicit hypotheses along the way.

The purpose of a baseline model is to:

- It gives us a lower bound on expected model performance.
- The tighter the lower bound, the more useful the baseline

## Why CNN for images?

Before selecting our baseline model, I want to explore the significance of Convolutional Neural Networks (CNNs) in image classification and understand why traditional feed-forward networks are less effective for this task.
"""

# Input shape (e.g., 224x224 RGB images flattened into a vector)
input_shape = (224 * 224 * 3,)

# Define the input
input_layer = layers.Input(shape=input_shape)

# Feed-forward neural network
x = layers.Dense(1024, activation='relu')(input_layer)
x = layers.Dense(512, activation='relu')(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.Dense(128, activation='relu')(x)

# First output: 'Class' (Multi-class classification, use 40 units)
class_output = layers.Dense(40, activation='softmax', name='class_output')(x)

# Second output: 'MoreThanOnePerson' (Binary classification)
person_output = layers.Dense(1, activation='sigmoid', name='person_output')(x)

# Create the model with two outputs
model = models.Model(inputs=input_layer, outputs=[class_output, person_output])

# Compile the model
model.compile(optimizer='adam',
              loss={'class_output': 'categorical_crossentropy',
                    'person_output': 'binary_crossentropy'},
              metrics={'class_output': 'accuracy',
                       'person_output': 'accuracy'})

# Print the model summary
model.summary()

# Ensure the input data is reshaped to match the input shape of the model
X_train_flattened = X_train.reshape(X_train.shape[0], 224 * 224 * 3) # image shapes
X_val_flattened = X_val.reshape(X_val.shape[0], 224 * 224 * 3)
X_test_flattened = X_test.reshape(X_test.shape[0], 224 * 224 * 3)

# Define the TensorBoard callback if you were using it in the CNN training
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='logs/ffnn', histogram_freq=1)

# Train the FFNN model
history_ffnn = model.fit(
    X_train_flattened,
    {'class_output': y_train_class_one_hot, 'person_output': y_train_humans},
    validation_data=(X_val_flattened, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans}),
    epochs=10,
    batch_size=32,
    callbacks=[tensorboard_callback]
)

"""- The loss function for predicting what the person is doing is Categorical Crossentropy. This loss function is used when we have one-hot encoded targets and we're predicting multiple classes. It measures the difference between the predicted probabilities and the true one-hot encoded labels.
- The loss function for predicting whether there are multiple people in the image is Binary Crossentropy. This loss function is used when we're predicting a binary outcome (e.g., 0 or 1). It measures the difference between the predicted probabilities for a single class and the actual binary labels.
- From the loss, we can see that gradient descent is struggling to find the minimum which indicates that the model is doing poorly for both targets. The loss is very high with values bigger than 4. Even with 154,836,009 parameters, the model is not even capable of overfitting the data which indicate that the architecture itself is poor for this task.

Lets see how the model genralize, the following plot is the total loss for both classification tasks:
"""

plt.style.use("ggplot")
plt.figure(figsize=(10,5))
Nepoch=10
plt.subplot(1,2,1)
plt.plot(np.arange(0, Nepoch), history_ffnn.history["loss"], label="train_loss")
plt.plot(np.arange(0, Nepoch), history_ffnn.history["val_loss"], label="validation_loss")
plt.title("Training Loss on Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss")
plt.legend(loc="upper right")
plt.show()

"""-  The training loss decreases sharply in the first epoch and then remains nearly constant. This indicates that the model quickly reached a point where it could fit the training data, but then made no significant further improvements.
- The validation loss remains nearly constant and doesn't decrease in the same way as the training loss. This suggests that the model is not generalizing well to the validation data.

## Baseline Model

Our next objective is to establish a comprehensive training and evaluation framework, ensuring its accuracy through a series of experiments. At this point, it's advisable to start with a straightforward model—something simple and foolproof, like a linear classifier or a minimal ConvNet. We will train this model, visualize the losses, and monitor other relevant metrics, such as accuracy. Additionally, we’ll examine model predictions and conduct a series of ablation experiments with clearly defined hypotheses to verify the model’s behavior along the way.

a “fast and furious” approach to training neural networks does not work and only leads to suffering. Now, suffering is a perfectly natural part of getting a neural network to work well, but it can be mitigated by being thorough, defensive, paranoid, and obsessed with visualizations of basically every possible thing. The qualities that in my experience correlate most strongly to success in deep learning are patience and attention to detail.

it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration.
"""

# Input shape (e.g., 64x64 RGB images)
input_shape = (224, 224, 3)

# Start with an input layer
inputs = layers.Input(shape=input_shape)

# First convolutional layer
x1 = layers.Conv2D(1, (3, 3), activation='relu')(inputs)

# # Flatten the output and add a fully connected layer
x2 = layers.Flatten()(x1)
x3 = layers.Dense(1, activation='relu')(x2)

# # First output: 'Class' (Multi-class classification, use 40 units)
class_output = layers.Dense(40, activation='softmax', name='class_output')(x3)

# # Second output: 'MoreThanOnePerson' (Binary classification)
person_output = layers.Dense(1, activation='sigmoid', name='person_output')(x3)

# # Create the model with two outputs
tiny_model = models.Model(inputs=inputs, outputs=[class_output, person_output])


# # Compile the model with the correct loss functions
tiny_model.compile(optimizer='adam',
                   loss={'class_output': 'categorical_crossentropy',
                         'person_output': 'binary_crossentropy'},
                   metrics={'class_output': 'accuracy',
                            'person_output': 'accuracy'})

# # Print the model summary
tiny_model.summary()

# Train the model
history_tiny_model = tiny_model.fit(
    X_train,
    {'class_output': y_train_class_one_hot, 'person_output': y_train_humans},
    epochs=10,  # Number of epochs to train
    batch_size=32,  # Size of each batch of data
    validation_data=(X_val, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans}))

"""- The model is underfitting which is expected as we wanted to start with a simple model and then add complexity."""

plt.style.use("ggplot")
plt.figure(figsize=(10,5))
Nepoch=10
plt.subplot(1,2,1)
plt.plot(np.arange(0, Nepoch), history_tiny_model.history["loss"], label="train_loss")
plt.plot(np.arange(0, Nepoch), history_tiny_model.history["val_loss"], label="validation_loss")
plt.title("Training Loss on Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss")
plt.legend(loc="upper right")
plt.show()

"""- The loss values (both training and validation) are quite high and very close to each other, indicating that the model is not learning effectively from the data.

## Why Not Just Increase Neural Network Depth
"""

def build_large_cnn(input_shape, num_classes):
    inputs = layers.Input(shape=input_shape)

    # Initial convolutional block
    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)
    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = layers.MaxPooling2D((2, 2), padding='same')(x)

    # Increasing the depth of the network
    for i in range(5):  # Reduce the number of convolutional blocks
        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
        x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)
        x = layers.MaxPooling2D((2, 2), padding='same')(x)

    for i in range(5):  # Reduce the number of convolutional blocks
        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)
        x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)
        x = layers.MaxPooling2D((2, 2), padding='same')(x)

    for i in range(3):  # Reduce the number of convolutional blocks
        x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)
        x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)
        x = layers.MaxPooling2D((2, 2), padding='same')(x)

    # Flatten the output
    x = layers.Flatten()(x)
    x = layers.Dense(4096, activation='relu')(x)
    x = layers.Dense(4096, activation='relu')(x)

    # First output: 'Class' (Multi-class classification, use num_classes units)
    class_output = layers.Dense(num_classes, activation='softmax', name='class_output')(x)

    # Second output: 'MoreThanOnePerson' (Binary classification)
    person_output = layers.Dense(1, activation='sigmoid', name='person_output')(x)

    # Create the model with two outputs
    model = models.Model(inputs=inputs, outputs=[class_output, person_output])

    return model

# Example usage
input_shape = (224, 224, 3)  # Example input shape for image data
num_classes = 40  # Example number of output classes for the multi-class classification
large_cnn_model = build_large_cnn(input_shape, num_classes)

# Compile the model with the correct loss functions
large_cnn_model.compile(optimizer='adam',
                        loss={'class_output': 'categorical_crossentropy',
                              'person_output': 'binary_crossentropy'},
                        metrics={'class_output': 'accuracy',
                                 'person_output': 'accuracy'})

# Model summary
large_cnn_model.summary()

# Train the model
history = large_cnn_model.fit(
    X_train,
    {'class_output': y_train_class_one_hot, 'person_output': y_train_humans},
    epochs=20,  # Number of epochs to train
    batch_size=32,  # Size of each batch of data
    validation_data=(X_val, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans}))

"""Deeper neural networks, while powerful in their ability to learn complex patterns, often face significant challenges that can hinder their performance. One major issue is the vanishing gradient problem, where the gradients used for updating the weights during backpropagation become exceedingly small as they propagate through many layers. This can cause the early layers in the network to learn very slowly or not at all, leading to suboptimal performance. Another challenge is the degradation problem, where simply adding more layers to a network results in higher training error, contrary to the expectation that a deeper network should perform better. This occurs because the network struggles to learn effective representations as it becomes more difficult to optimize the deeper layers. Additionally, deeper networks are more prone to overfitting, where the model memorizes the training data instead of generalizing well to unseen data, particularly when the dataset is not sufficiently large or diverse. These problems highlight the need for architectural innovations, such as residual connections in ResNet, to facilitate the training of deeper networks by improving gradient flow and enabling easier optimization.

## Overfitting model

- At this stage we should have a good understanding of the dataset and we have the full training + evaluation pipeline working. For any given model we can (reproducibly) compute a metric that we trust. We are also armed with our performance for an input-independent baseline, the performance of a few dumb baselines (we better beat these).

Our approach would be to first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration.

- To reach a good training loss we'll want to choose an appropriate architecture for the data. When it comes to choosing this I always avoid to be a "hero". Getting eager to get crazy and creative in stacking up the lego blocks of the neural net toolbox in various exotic architectures that make sense to us wont work. We need to Resist this temptation strongly in the early stages of our project. We need to find the most related paper and copy paste their simplest architecture that achieves good performance. E.g. since we are classifying images we don't need to be hero and we just copy paste a ResNet-50 for our first run. We are allowed to do something more custom later and beat this.

- adam is safe. In the early stages of setting baselines I like to use Adam with a learning rate of 3e-4. Adam is much more forgiving to hyperparameters, including a bad learning rate. For ConvNets a well-tuned SGD will almost always slightly outperform Adam, but the optimal learning rate region is much more narrow and problem-specific. (Note: If we are using RNNs and related sequence models it is more common to use Adam. At the initial stage of your project, again, we don’t want to be heros and follow whatever the most related papers do.)


Lets try different CNN architectures and see which one perform best without regularizations:

### ResNet50

ResNet, or Residual Networks, addresses the challenges of training deep neural networks, particularly the vanishing gradient and degradation problems, through the introduction of residual connections. These connections allow the network to learn residual functions, which are the differences between the desired output and the input, rather than learning the output directly. By doing so, ResNet enables the gradients to flow more easily through the network during backpropagation, effectively mitigating the vanishing gradient problem. This also helps in addressing the degradation problem, as the residual connections allow layers to skip over unnecessary complexity if needed, ensuring that the network's performance does not degrade as its depth increases. These skip connections make it easier to optimize very deep networks, enabling ResNet to achieve state-of-the-art performance while maintaining or improving accuracy as the network depth grows. This architectural innovation has allowed ResNet to train networks with hundreds or even thousands of layers without suffering from the issues typically associated with deep neural networks.
"""

# Input shape (e.g., 224x224 RGB images)
input_shape = (224, 224, 3)

# Load the ResNet50 model pre-trained on ImageNet, excluding the top layers
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)

# Add a global average pooling layer (this is part of the original architecture)
x = GlobalAveragePooling2D()(base_model.output)

# First output: 'Class' (Multi-class classification, use 40 units)
class_output = Dense(40, activation='softmax', name='class_output')(x)

# Second output: 'MoreThanOnePerson' (Binary classification)
person_output = Dense(1, activation='sigmoid', name='person_output')(x)

# Create the model with two outputs
model = tf.keras.models.Model(inputs=base_model.input, outputs=[class_output, person_output])

# Define the learning rate
learning_rate = 3e-4

# Create the Adam optimizer with the specified learning rate
optimizer = Adam(learning_rate=learning_rate)

# Compile the model with the optimizer and loss functions
model.compile(optimizer=optimizer,
              loss={'class_output': 'categorical_crossentropy',
                    'person_output': 'binary_crossentropy'},
              metrics={'class_output': 'accuracy',
                       'person_output': 'accuracy'})

# Print the model summary
model.summary()

# Train the model
history = model.fit(
    X_train,
    {'class_output': y_train_class_one_hot, 'person_output': y_train_humans},
    epochs=10,  # Number of epochs to train
    batch_size=32,  # Size of each batch of data
    validation_data=(X_val, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans})
)

"""We can see that ResNet solves the issue of the vanishing gradient and degradation problems but the model is highly overfitting. Lets try different architectures.

### VGG16

VGG16 tackles the challenge of deeper networks by using a straightforward and uniform architecture, which consists of 16 layers with small 3x3 convolutional filters stacked one after the other. This design choice allows VGG16 to increase network depth while keeping the complexity of each layer manageable, ensuring that the network can learn more detailed and hierarchical features. Unlike ResNet, which introduces residual connections to allow the network to bypass certain layers and mitigate the vanishing gradient and degradation problems, VGG16 relies solely on increasing depth through sequential layers without skip connections. As a result, VGG16 may encounter difficulties in training very deep networks, as the lack of residual connections means it does not directly address issues like vanishing gradients or degradation in performance as the network deepens. In essence, while VGG16 demonstrates the effectiveness of deeper networks through its simple and consistent design, it does not incorporate the architectural innovations found in ResNet that facilitate the training of very deep networks by improving gradient flow and network optimization.
"""

# Load pre-trained VGG16 without the top layers
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model layers
base_model.trainable = False

# Build your model on top of the base model
inputs = base_model.input
x = base_model.output
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(256, activation='relu')(x)
# x = layers.Dropout(0.5)(x)

# Output layers
class_output = layers.Dense(40, activation='softmax', name='class_output')(x)
person_output = layers.Dense(1, activation='sigmoid', name='person_output')(x)

# Define the model
transfer_model = models.Model(inputs=inputs, outputs=[class_output, person_output])


optimizer = optimizers.Adam(learning_rate=3e-4)  # Increased learning rate
transfer_model.compile(
    optimizer=optimizer,
    loss={
        'class_output': 'categorical_crossentropy',
        'person_output': 'binary_crossentropy'
    },
    metrics={
        'class_output': 'accuracy',
        'person_output': 'accuracy'
    }
)


# Model summary
transfer_model.summary()

# Train the model
transfer_model = transfer_model.fit(
    X_train,
    {'class_output': y_train_class_one_hot, 'person_output': y_train_humans},
    epochs=100,  # Number of epochs to train
    batch_size=32,  # Size of each batch of data
    validation_data=(X_val, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans})
)

"""### MobileNetV2

MobileNet is a family of neural network architectures designed specifically for efficient performance on mobile and embedded devices. Introduced by Google, MobileNet prioritizes both speed and low computational cost while maintaining strong performance in tasks like image classification and object detection. The key innovation in MobileNet lies in its use of depthwise separable convolutions, which significantly reduce the number of parameters and computational complexity compared to standard convolutional layers.

MobileNet might be helpful to reduce overfitting.
"""

# Load pre-trained MobileNetV2 without the top layers
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model layers
base_model.trainable = False

# Build your model on top of the base model
inputs = base_model.input
x = base_model.output
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(128, activation='relu')(x)  # Using 128 units in the dense layer for a smaller model
# x = layers.Dropout(0.5)(x)

# Output layers
class_output = layers.Dense(40, activation='softmax', name='class_output')(x)
person_output = layers.Dense(1, activation='sigmoid', name='person_output')(x)

# Define the model
mobilenet_model = models.Model(inputs=inputs, outputs=[class_output, person_output])

# Compile the model
optimizer = optimizers.Adam(learning_rate=1e-4)  # Adjust learning rate as needed
mobilenet_model.compile(
    optimizer=optimizer,
    loss={
        'class_output': 'categorical_crossentropy',
        'person_output': 'binary_crossentropy'
    },
    metrics={
        'class_output': 'accuracy',
        'person_output': 'accuracy'
    }
)

# Model summary
mobilenet_model.summary()

# Train the model
mobilenet_model = mobilenet_model.fit(
    X_train,
    {'class_output': y_train_class_one_hot, 'person_output': y_train_humans},
    epochs=20,  # Number of epochs to train
    batch_size=32,  # Size of each batch of data
    validation_data=(X_val, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans})
)

"""### NasNetLarge

We can see that we are struggling to find the right neural architecture. NasNetLarge is a neural network architecture developed by Google that was discovered using Neural Architecture Search (NAS), an automated process that searches for the best possible neural network architecture for a given task. The key idea behind NAS is to automate the design of neural networks, optimizing the architecture itself based on the target performance metrics, such as accuracy and computational efficiency.
"""

# Load pre-trained NASNetLarge without the top layers
base_model = NASNetLarge(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model layers
base_model.trainable = False

# Build your model on top of the base model
inputs = base_model.input
x = base_model.output
x = layers.GlobalAveragePooling2D()(x)
x = layers.Dense(256, activation='relu')(x)
# x = layers.Dropout(0.5)(x)

# Output layers
class_output = layers.Dense(40, activation='softmax', name='class_output')(x)
person_output = layers.Dense(1, activation='sigmoid', name='person_output')(x)

# Define the model
nasnet_model = models.Model(inputs=inputs, outputs=[class_output, person_output])

# Compile the model
optimizer = optimizers.Adam(learning_rate=1e-4)
nasnet_model.compile(
    optimizer=optimizer,
    loss={
        'class_output': 'categorical_crossentropy',
        'person_output': 'binary_crossentropy'
    },
    metrics={
        'class_output': 'accuracy',
        'person_output': 'accuracy'
    }
)

# Model summary
nasnet_model.summary()

# Train the model
nasnet_model = nasnet_model.fit(
    X_train,
    {'class_output': y_train_class_one_hot, 'person_output': y_train_humans},
    epochs=20,  # Number of epochs to train
    batch_size=32,  # Size of each batch of data
    validation_data=(X_val, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans})
)

"""# MobileNet Regularization

## Increasing data size via data augemntation
"""

# Define data augmentation parameters
datagen = ImageDataGenerator(
    rotation_range=20,      # Randomly rotate images in the range (degrees, 0 to 180)
    width_shift_range=0.2,  # Randomly shift images horizontally (fraction of total width)
    height_shift_range=0.2, # Randomly shift images vertically (fraction of total height)
    shear_range=0.2,        # Shear intensity (shear angle in counter-clockwise direction in degrees)
    zoom_range=0.2,         # Randomly zoom in/out on images
    horizontal_flip=True,   # Randomly flip images horizontally
    fill_mode='nearest'     # Fill mode for new pixels (nearest, reflect, wrap, constant)
)

# Apply the transformations to the training set
datagen.fit(X_train)

num_augmentations = 5  # Number of augmentations per image
augmented_images = []
augmented_labels_class = []
augmented_labels_humans = []

for img, label_class, label_humans in zip(X_train, y_train_class_one_hot, y_train_humans):
    # First, add the original image and its labels
    augmented_images.append(img)
    augmented_labels_class.append(label_class)
    augmented_labels_humans.append(label_humans)

    # Then, add the augmented images and their labels
    for i in range(num_augmentations):
        augmented_img = datagen.random_transform(img)
        augmented_images.append(augmented_img)
        augmented_labels_class.append(label_class)
        augmented_labels_humans.append(label_humans)

# Convert lists to arrays
augmented_images = np.array(augmented_images)
augmented_labels_class = np.array(augmented_labels_class)
augmented_labels_humans = np.array(augmented_labels_humans)

augmented_images.shape

X_train.shape

2700 * 6 # (original images + 5 augmentations per image)

"""## dropouts, L1 & L2

- apply different drop outs
- weight regularization L1 and L2 and L1 + L2
"""

def create_mobilenet_model(dropout_rate=0.5, l1_reg=0.0, l2_reg=0.0):
    # Load pre-trained MobileNet without the top layers
    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    # Freeze the base model layers
    base_model.trainable = False

    # Build your model on top of the base model
    inputs = base_model.input
    x = base_model.output
    x = layers.GlobalAveragePooling2D()(x)

    # Add Dense layer with regularization
    x = layers.Dense(256,
                     activation='relu',
                     kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)

    # Add Dropout layer
    x = layers.Dropout(dropout_rate)(x)

    # Output layers
    class_output = layers.Dense(40, activation='softmax', name='class_output')(x)
    person_output = layers.Dense(1, activation='sigmoid', name='person_output')(x)

    # Define the model
    model = models.Model(inputs=inputs, outputs=[class_output, person_output])

    return model

# Define the hyperparameter ranges
dropout_rates = [0.3, 0.5, 0.7]
l1_regs = [0.0, 1e-5, 1e-4]
l2_regs = [0.0, 1e-5, 1e-4]

# Variables to keep track of the best hyperparameters
best_val_loss = float('inf')
best_hyperparameters = {}

# Iterate over each combination of hyperparameters
for dropout_rate in dropout_rates:
    for l1_reg in l1_regs:
        for l2_reg in l2_regs:
            # Create a new model with the specified hyperparameters
            mobilenet_model = create_mobilenet_model(dropout_rate=dropout_rate, l1_reg=l1_reg, l2_reg=l2_reg)

            # Compile the model
            optimizer = optimizers.Adam(learning_rate=1e-4)
            mobilenet_model.compile(
                optimizer=optimizer,
                loss={
                    'class_output': 'categorical_crossentropy',
                    'person_output': 'binary_crossentropy'
                },
                metrics={
                    'class_output': 'accuracy',
                    'person_output': 'accuracy'
                }
            )

            # Set up callbacks for early stopping and model checkpointing
            early_stopping = EarlyStopping(
                monitor='val_loss',
                patience=5,
                restore_best_weights=True
            )

            checkpoint = ModelCheckpoint(
                f"mobilenet_model_d{dropout_rate}_l1{l1_reg}_l2{l2_reg}.weights.h5",
                monitor='val_loss',
                save_best_only=True,
                save_weights_only=True
            )

            # Train the model using the augmented dataset
            history = mobilenet_model.fit(
                augmented_images,
                {'class_output': augmented_labels_class, 'person_output': augmented_labels_humans},
                epochs=20,  # Adjust the number of epochs as needed
                batch_size=32,  # Size of each batch of data
                validation_data=(X_val, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans}),
                callbacks=[early_stopping, checkpoint]  # Include the callbacks here
            )

            # Get the best validation loss from this run
            min_val_loss = min(history.history['val_loss'])

            # Check if this is the best validation loss we've seen
            if min_val_loss < best_val_loss:
                best_val_loss = min_val_loss
                best_hyperparameters = {
                    'dropout_rate': dropout_rate,
                    'l1_reg': l1_reg,
                    'l2_reg': l2_reg
                }

# Print the best hyperparameters
print("Best hyperparameters:")
print(f"Dropout Rate: {best_hyperparameters['dropout_rate']}")
print(f"L1 Regularization: {best_hyperparameters['l1_reg']}")
print(f"L2 Regularization: {best_hyperparameters['l2_reg']}")
print(f"Best Validation Loss: {best_val_loss}")

"""Best hyperparameters:

- Dropout Rate: 0.5
- L1 Regularization: 0.0
- L2 Regularization: 1e-05

This achieved Validation Loss: 1.348402738571167
"""

def create_nasnet_model(dropout_rate=0.5, l1_reg=0.0, l2_reg=0.0):
    # Load pre-trained NASNetMobile without the top layers
    base_model = NASNetMobile(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    # Freeze the base model layers
    base_model.trainable = False

    # Build your model on top of the base model
    inputs = base_model.input
    x = base_model.output
    x = layers.GlobalAveragePooling2D()(x)

    # Add Dense layer with regularization
    x = layers.Dense(256,
                     activation='relu',
                     kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)

    # Add Dropout layer
    x = layers.Dropout(dropout_rate)(x)

    # Output layers
    class_output = layers.Dense(40, activation='softmax', name='class_output')(x)
    person_output = layers.Dense(1, activation='sigmoid', name='person_output')(x)

    # Define the model
    model = models.Model(inputs=inputs, outputs=[class_output, person_output])

    return model

# Define the hyperparameter ranges
dropout_rates = [0.3, 0.5, 0.7]
l1_regs = [0.0, 1e-5, 1e-4]
l2_regs = [0.0, 1e-5, 1e-4]

# Variables to keep track of the best hyperparameters
best_val_loss = float('inf')
best_hyperparameters = {}

# Iterate over each combination of hyperparameters
for dropout_rate in dropout_rates:
    for l1_reg in l1_regs:
        for l2_reg in l2_regs:
            # Create a new model with the specified hyperparameters
            nasnet_model = create_nasnet_model(dropout_rate=dropout_rate, l1_reg=l1_reg, l2_reg=l2_reg)

            # Compile the model
            optimizer = optimizers.Adam(learning_rate=1e-4)
            nasnet_model.compile(
                optimizer=optimizer,
                loss={
                    'class_output': 'categorical_crossentropy',
                    'person_output': 'binary_crossentropy'
                },
                metrics={
                    'class_output': 'accuracy',
                    'person_output': 'accuracy'
                }
            )

            # Set up callbacks for early stopping and model checkpointing
            early_stopping = EarlyStopping(
                monitor='val_loss',
                patience=5,
                restore_best_weights=True
            )

            checkpoint = ModelCheckpoint(
                f"nasnet_model_d{dropout_rate}_l1{l1_reg}_l2{l2_reg}.weights.h5",
                monitor='val_loss',
                save_best_only=True,
                save_weights_only=True
            )

            # Train the model using the augmented dataset
            history = nasnet_model.fit(
                augmented_images,
                {'class_output': augmented_labels_class, 'person_output': augmented_labels_humans},
                epochs=100,  # Adjust the number of epochs as needed
                batch_size=32,  # Size of each batch of data
                validation_data=(X_val, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans}),
                callbacks=[early_stopping, checkpoint]  # Include the callbacks here
            )

            # Get the best validation loss from this run
            min_val_loss = min(history.history['val_loss'])

            # Check if this is the best validation loss we've seen
            if min_val_loss < best_val_loss:
                best_val_loss = min_val_loss
                best_hyperparameters = {
                    'dropout_rate': dropout_rate,
                    'l1_reg': l1_reg,
                    'l2_reg': l2_reg
                }

# Print the best hyperparameters
print("Best hyperparameters:")
print(f"Dropout Rate: {best_hyperparameters['dropout_rate']}")
print(f"L1 Regularization: {best_hyperparameters['l1_reg']}")
print(f"L2 Regularization: {best_hyperparameters['l2_reg']}")
print(f"Best Validation Loss: {best_val_loss}")

"""## Optimizer regularization"""

def create_mobilenet_model(dropout_rate=0.7, l1_reg=0.0, l2_reg=1e-5):
    # Load pre-trained MobileNet without the top layers
    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    # Freeze the base model layers
    base_model.trainable = False

    # Build your model on top of the base model
    inputs = base_model.input
    x = base_model.output
    x = layers.GlobalAveragePooling2D()(x)

    # Add Dense layer with regularization
    x = layers.Dense(256,
                     activation='relu',
                     kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)

    # Add Dropout layer
    x = layers.Dropout(dropout_rate)(x)

    # Add Batch Normalization
    x = layers.BatchNormalization()(x)

    # Output layers
    class_output = layers.Dense(40, activation='softmax', name='class_output')(x)
    person_output = layers.Dense(1, activation='sigmoid', name='person_output')(x)

    # Define the model
    model = models.Model(inputs=inputs, outputs=[class_output, person_output])

    return model

# Define the optimizers
optimizers_dict = {
    'SGD': optimizers.SGD(learning_rate=1e-4),
    'Momentum': optimizers.SGD(learning_rate=1e-4, momentum=0.9),
    'Adam': optimizers.Adam(learning_rate=1e-4),
    'RMSprop': optimizers.RMSprop(learning_rate=1e-4),
    'Adaptive': optimizers.Adam(learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=1e-4,
        decay_steps=100000,
        decay_rate=0.96,
        staircase=True))
}

# Set up callbacks for early stopping and model checkpointing
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True
)

# Variables to track the best optimizer and validation loss
best_val_loss = float('inf')
best_optimizer_name = None

# Train the model with each optimizer
for optimizer_name, optimizer in optimizers_dict.items():
    print(f"Training with {optimizer_name} optimizer...")

    # Create a new model instance
    model = create_mobilenet_model(dropout_rate=0.5, l1_reg=0.0, l2_reg=1e-5)

    # Compile the model with the current optimizer
    model.compile(
        optimizer=optimizer,
        loss={
            'class_output': 'categorical_crossentropy',
            'person_output': 'binary_crossentropy'
        },
        metrics={
            'class_output': 'accuracy',
            'person_output': 'accuracy'
        }
    )

    # Define model checkpoint for each optimizer
    checkpoint = ModelCheckpoint(
        f"mobilenet_model_{optimizer_name}.weights.h5",
        monitor='val_loss',
        save_best_only=True,
        save_weights_only=True
    )

    # Train the model
    history = model.fit(
        augmented_images,
        {'class_output': augmented_labels_class, 'person_output': augmented_labels_humans},
        epochs=20,  # Adjust the number of epochs as needed
        batch_size=32,  # Size of each batch of data
        validation_data=(X_val, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans}),
        callbacks=[early_stopping, checkpoint]  # Include the callbacks here
    )

    # Get the best validation loss from this run
    min_val_loss = min(history.history['val_loss'])

    # Update the best optimizer if this one performs better
    if min_val_loss < best_val_loss:
        best_val_loss = min_val_loss
        best_optimizer_name = optimizer_name

    print(f"{optimizer_name} optimizer achieved a minimum validation loss of {min_val_loss}\n")

# Print the best optimizer
print(f"The best optimizer is: {best_optimizer_name} with a validation loss of {best_val_loss}")

"""# Saving the final model"""

def create_mobilenet_model(dropout_rate=0.3, l1_reg=0.0, l2_reg=0.0):
    # Load pre-trained MobileNet without the top layers
    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    # Freeze the base model layers
    base_model.trainable = False

    # Build your model on top of the base model
    inputs = base_model.input
    x = base_model.output
    x = layers.GlobalAveragePooling2D()(x)

    # Add Dense layer with regularization
    x = layers.Dense(256,
                     activation='relu',
                     kernel_regularizer=regularizers.l1_l2(l1=l1_reg, l2=l2_reg))(x)

    # Add Dropout layer
    x = layers.Dropout(dropout_rate)(x)

    # Output layers
    class_output = layers.Dense(40, activation='softmax', name='class_output')(x)
    person_output = layers.Dense(1, activation='sigmoid', name='person_output')(x)

    # Define the model
    model = models.Model(inputs=inputs, outputs=[class_output, person_output])

    return model

# Create the model with the specified hyperparameters
model = create_mobilenet_model(dropout_rate=0.5, l1_reg=0.0, l2_reg=1e-05)

# Compile the model with Adam optimizer
optimizer = optimizers.Adam(learning_rate=1e-4)
model.compile(
    optimizer=optimizer,
    loss={
        'class_output': 'categorical_crossentropy',
        'person_output': 'binary_crossentropy'
    },
    metrics={
        'class_output': 'accuracy',
        'person_output': 'accuracy'
    }
)

# Model summary
model.summary()

# Set up callbacks for early stopping and model checkpointing
early_stopping = EarlyStopping(
    monitor='val_loss',
    patience=10,  # Stop early with low patience to catch the intersection point
    restore_best_weights=True
)

# Modify ModelCheckpoint to save the entire model, with .keras extension
checkpoint = ModelCheckpoint(
    "mobilenet_model_adam_best.keras",
    monitor='val_loss',
    save_best_only=True,
    save_weights_only=False  # Save the entire model, not just the weights
)

# Train the model using the augmented dataset
history = model.fit(
    augmented_images,
    {'class_output': augmented_labels_class, 'person_output': augmented_labels_humans},
    epochs=100,  # Maximum number of epochs
    batch_size=32,  # Size of each batch of data
    validation_data=(X_val, {'class_output': y_val_class_one_hot, 'person_output': y_val_humans}),
    callbacks=[early_stopping, checkpoint]  # Include the callbacks here
)

# Save the best hyperparameters and other details
best_hyperparameters = {
    "dropout_rate": 0.5,
    "l1_reg": 0.0,
    "l2_reg": 1e-05,
    "optimizer": "Adam",
    "learning_rate": 1e-4,
    "best_val_loss": min(history.history['val_loss'])
}

with open("best_model_hyperparameters.json", "w") as f:
    json.dump(best_hyperparameters, f)

print(f"Best model saved with validation loss: {best_hyperparameters['best_val_loss']}")

plt.style.use("ggplot")
plt.figure(figsize=(10,5))
Nepoch=29
plt.subplot(1,2,1)
plt.plot(np.arange(0, Nepoch), history.history["loss"], label="train_loss")
plt.plot(np.arange(0, Nepoch), history.history["val_loss"], label="validation_loss")
plt.title("Training Loss on Dataset")
plt.xlabel("Epoch #")
plt.ylabel("Loss")
plt.legend(loc="upper right")
plt.show()

# Assuming `history` is the History object returned by the `fit` method
# Replace Nepoch with the actual number of epochs

Nepoch = 29 # Number of epochs

plt.style.use("ggplot")
plt.figure(figsize=(12, 6))

# Plotting Person Classification Accuracy (more than one person)
plt.subplot(1, 2, 1)
plt.plot(np.arange(0, Nepoch), history.history["person_output_accuracy"], label="train_person_accuracy")
plt.plot(np.arange(0, Nepoch), history.history["val_person_output_accuracy"], label="val_person_accuracy")
plt.title("Person Classification Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Accuracy")
plt.legend(loc="lower right")

# Plotting Activity Classification Accuracy (what the person is doing)
plt.subplot(1, 2, 2)
plt.plot(np.arange(0, Nepoch), history.history["class_output_accuracy"], label="train_class_accuracy")
plt.plot(np.arange(0, Nepoch), history.history["val_class_output_accuracy"], label="val_class_accuracy")
plt.title("Activity Classification Accuracy")
plt.xlabel("Epoch #")
plt.ylabel("Accuracy")
plt.legend(loc="lower right")

plt.show()

"""# Model Test"""

import tensorflow as tf
import numpy as np
from sklearn.metrics import classification_report, accuracy_score

# Load the saved model
model = tf.keras.models.load_model("mobilenet_model_adam_best.keras")

# Predict the classes and the person count for the test set
test_predictions = model.predict(X_test)

# Extract individual predictions
test_class_predictions = np.argmax(test_predictions[0], axis=1)  # Activity class predictions
test_person_predictions = (test_predictions[1] > 0.5).astype(int)  # MoreThanOnePerson predictions

# Decode the class predictions using the mapping dictionary
decoded_test_class_predictions = [mapping[pred] for pred in test_class_predictions]

# Convert binary predictions for 'MoreThanOnePerson' to 'YES'/'NO'
decoded_test_person_predictions = np.where(test_person_predictions == 1, 'YES', 'NO')

# Convert binary values in y_test_humans to 'YES'/'NO'
y_test_humans_categorical = np.where(y_test_humans == 1, 'YES', 'NO')

# Evaluate the performance by comparing predictions to ground truth
print("Classification Report for Class Predictions:")
print(classification_report([mapping[i] for i in y_test_class], decoded_test_class_predictions))

# Evaluate 'MoreThanOnePerson' binary classification performance
print("Classification Report for 'MoreThanOnePerson' Predictions:")
print(classification_report(y_test_humans_categorical, decoded_test_person_predictions))

# Optionally, compute accuracy for both tasks
accuracy_class = accuracy_score([mapping[i] for i in y_test_class], decoded_test_class_predictions)
accuracy_person = accuracy_score(y_test_humans_categorical, decoded_test_person_predictions)

print(f"Test Accuracy for Class Predictions: {accuracy_class:.4f}")
print(f"Test Accuracy for 'MoreThanOnePerson' Predictions: {accuracy_person:.4f}")

"""### **Class Predictions:**
- **Overall Accuracy**: The test accuracy for class predictions is **71%**.
- **Best Performing Classes**:
  - **Climbing**: High precision, recall, and F1-score of **0.94** across the board.
  - **Riding a Bike**: Almost perfect precision and recall, with **0.97** precision and **1.00** recall.
  - **Rowing a Boat**: Perfect precision, recall, and F1-score of **1.00**.
  - **Playing Violin**: High precision and recall with an F1-score of **0.88**.
  - **Feeding a Horse** and **Playing Guitar**: High F1-scores of **0.86** and **0.88**, respectively.
- **Weaker Performing Classes**:
  - **Waving Hands**: Very low performance with an F1-score of **0.09**.
  - **Texting Message**: Low F1-score of **0.17** with precision at **0.22** and recall at **0.14**.
  - **Smoking**: Lower performance with an F1-score of **0.40**.
- **Macro Avg**: Precision, recall, and F1-score hover around **0.69**.
- **Weighted Avg**: Precision, recall, and F1-score are all around **0.70**.

### **MoreThanOnePerson (Binary Classification):**
- **Overall Accuracy**: The test accuracy for "MoreThanOnePerson" predictions is **78.89%**.
- **Class 'NO'** (Single person in the image):
  - **Precision**: **0.81**
  - **Recall**: **0.88**
  - **F1-Score**: **0.84**
- **Class 'YES'** (More than one person in the image):
  - **Precision**: **0.75**
  - **Recall**: **0.64**
  - **F1-Score**: **0.69**
- **Macro Avg**: Precision, recall, and F1-score are approximately **0.78**, **0.76**, and **0.77**, respectively.

### **Key Insights:**
- The model performs reasonably well with an overall accuracy of **71%** for class predictions.
- It excels in certain activities like **Climbing**, **Riding a Bike**, and **Rowing a Boat**, where it achieves very high precision and recall.
- However, some activities like **Waving Hands**, **Texting Message**, and **Smoking** have significantly lower precision and recall, suggesting that the model struggles with these activities.
- For the binary classification task ("MoreThanOnePerson"), the model performs well, with an accuracy close to **79%**. It predicts the "NO" class (single person) more accurately than the "YES" class (more than one person).

# Predictions
"""

import tensorflow as tf
import cv2
import os
import numpy as np
import pandas as pd

# Load the saved model
model = tf.keras.models.load_model("mobilenet_model_adam_best.keras")

# Standardize images
def standardize_image(image):
    standardized_image = cv2.resize(image, (224, 224))  # Resize to the model's input size
    standardized_image = standardized_image / 255.0  # Normalize pixel values to [0, 1]
    return standardized_image

# Function to load and preprocess images
def load_images_from_folder(folder, filenames):
    images = []
    for filename in filenames:
        img_path = os.path.join(folder, filename)
        img = cv2.imread(img_path)
        if img is not None:
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB
            img = standardize_image(img)
            images.append(img)
    return np.array(images)

# Load the CSV file containing the image filenames
predictions_df = pd.read_csv('A1_2024_data/s1234567_predictions.csv')

# Get the list of image filenames
image_filenames = predictions_df['FileName'].tolist()

# Load and preprocess the images
images = load_images_from_folder('Images', image_filenames)

# Predict the classes and the person count using the loaded model
predictions = model.predict(images)

# Extract the individual predictions
class_predictions = np.argmax(predictions[0], axis=1)  # Activity class predictions
person_predictions = (predictions[1] > 0.5).astype(int)  # MoreThanOnePerson predictions (threshold at 0.5)

# Use the mapping to convert numerical class predictions to category names
decoded_class_predictions = [mapping[pred] for pred in class_predictions]

# Convert the binary predictions of 'MoreThanOnePerson' to 'YES'/'NO'
decoded_person_predictions = np.where(person_predictions == 1, 'YES', 'NO')

# Write the predictions back to the DataFrame
predictions_df['Class'] = decoded_class_predictions
predictions_df['MoreThanOnePerson'] = decoded_person_predictions

# Save the updated DataFrame to the CSV file
predictions_df.to_csv('A1_2024_data/s3969393_predictions.csv', index=False)

print("Predictions saved to s3969393_predictions.csv")

"""# Conclusion

In this notebook, we embarked on a journey to build a CNN model for image classification, focusing on recognizing human activities and the presence of multiple people. We started with thorough data exploration, delving into the dataset's characteristics, identifying potential issues like class imbalances, and preprocessing the images to ensure consistency and efficiency during training.

We then established a baseline model and gradually increased complexity, culminating in the implementation of a ResNet50 architecture. While ResNet50 showed promising results, it still exhibited signs of overfitting.

Overfitting occurs when a model learns the training data too well, including its noise and specificities, hindering its ability to generalize to unseen data. To mitigate overfitting, we can consider several strategies:

Data Augmentation: Artificially expand the dataset by creating variations of existing images through techniques such as rotation, flipping, cropping, and color adjustments. This exposes the model to a wider range of data representations, improving its generalization ability.
Regularization Techniques: Introduce penalties for complex model parameters during training. Common methods include L1 or L2 regularization, which add a penalty to the loss function based on the size of the weights. Dropout, another regularization technique, randomly deactivates neurons during training, preventing the model from relying too heavily on any single neuron and encouraging it to learn more robust features.
Hyperparameter Tuning: Experiment with different hyperparameters, such as learning rate, batch size, and optimizer settings, to find the optimal configuration that balances model complexity and generalization.
Increase Training Data: While data augmentation is helpful, increasing the size and diversity of the original training dataset is crucial. This can involve collecting more images, ensuring a more balanced representation of classes, and incorporating images from different sources and environments.
By strategically employing these techniques, we can guide our model towards better generalization and improved performance on unseen data.

Increasing the volume and diversity of the training data is paramount in combating overfitting and enhancing model generalization. Here are several avenues to explore:

Gather More Images: Actively seek and collect more images relevant to the classification task. Consider using web scraping techniques or exploring publicly available image datasets.
Address Class Imbalances: Focus on collecting more images for underrepresented classes to create a more balanced dataset. This will prevent the model from being biased towards the majority classes.
Diversify Data Sources: Incorporate images from various sources and environments to ensure the model is exposed to a wider range of lighting conditions, backgrounds, and image variations. This will enhance its ability to generalize to different scenarios.
Utilize Unlabeled Data: Explore techniques like semi-supervised learning to leverage the unlabeled portion of the dataset. This can help the model learn more general features and improve its overall performance.
A larger and more diverse training dataset will equip the model with a broader understanding of the underlying patterns and improve its ability to generalize to new, unseen data, ultimately leading to more accurate and robust predictions.

# References

- ResNet:

He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. Proceedings of the IEEE conference on computer vision and pattern recognition, 770-778. Available at: https://arxiv.org/abs/1512.03385

- VGG16:

Simonyan, K., & Zisserman, A. (2015). Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations (ICLR). Available at: https://arxiv.org/abs/1409.1556

- MobileNet:

Keras Documentation. MobileNet Application. Available at: https://keras.io/api/applications/mobilenet/

- NASNet:

Keras Documentation. NASNet Application. Available at: https://keras.io/api/applications/nasnet/

Zoph, B., Vasudevan, V., Shlens, J., & Le, Q. V. (2018). Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 8697-8710). Available at: https://arxiv.org/abs/1707.07012


- Handling Overfitting in Deep Learning Models:

Towards Data Science. (2019). Handling Overfitting in Deep Learning Models. Available at: https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e

- Human Emotions Detection:

Aly Raafat. (2020). Human Emotions Detection. GitHub repository. Available at: https://github.com/alyraafat/Human-emotions-detection
"""